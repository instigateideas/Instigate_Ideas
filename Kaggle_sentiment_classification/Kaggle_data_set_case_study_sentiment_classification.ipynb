{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lemma=WordNetLemmatizer()\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk.data\n",
    "#nltk.download('punkt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'D:\\text_analytiics\\Kaggle_dataset_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (159571, 8)\n",
      "test shape:  (153164, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data_full=pd.read_csv('train.csv')\n",
    "test_data_full=pd.read_csv('test.csv')\n",
    "print('train shape: ', train_data_full.shape)\n",
    "print('test shape: ',test_data_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  00001cee341fdb12     -1            -1       -1      -1      -1   \n",
       "1  0000247867823ef7     -1            -1       -1      -1      -1   \n",
       "2  00013b17ad220c46     -1            -1       -1      -1      -1   \n",
       "3  00017563c3f7919a     -1            -1       -1      -1      -1   \n",
       "4  00017695ad8997eb     -1            -1       -1      -1      -1   \n",
       "\n",
       "   identity_hate  \n",
       "0             -1  \n",
       "1             -1  \n",
       "2             -1  \n",
       "3             -1  \n",
       "4             -1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the test data labels\n",
    "test_labels=pd.read_csv('test_labels.csv')\n",
    "print(test_labels.shape)\n",
    "test_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...     -1   \n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...     -1   \n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...     -1   \n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...     -1   \n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all.     -1   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0            -1       -1      -1      -1             -1  \n",
       "1            -1       -1      -1      -1             -1  \n",
       "2            -1       -1      -1      -1             -1  \n",
       "3            -1       -1      -1      -1             -1  \n",
       "4            -1       -1      -1      -1             -1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join test set with test labels\n",
    "test_data_with_labels=test_data_full.merge(test_labels, on=['id'], how='inner')\n",
    "test_data_with_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:  We can clearly see the number of class in test set is more than train set.  So for the model to fully understand the -1 category class we should have data points of -1 in train set. So combining train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique classes in train set in toxic class:  [0 1]\n",
      "The number of unique classes in test set in toxic class:  [-1  0  1]\n",
      "The number of unique classes in train set in severe_toxic class:  [0 1]\n",
      "The number of unique classes in test set in severe_toxic class:  [-1  0  1]\n",
      "The number of unique classes in train set in obscene class:  [0 1]\n",
      "The number of unique classes in test set in obscene class:  [-1  0  1]\n",
      "The number of unique classes in train set in threat class:  [0 1]\n",
      "The number of unique classes in test set in threat class:  [-1  0  1]\n",
      "The number of unique classes in train set in insult class:  [0 1]\n",
      "The number of unique classes in test set in insult class:  [-1  0  1]\n",
      "The number of unique classes in train set in identity_hate class:  [0 1]\n",
      "The number of unique classes in test set in identity_hate class:  [-1  0  1]\n"
     ]
    }
   ],
   "source": [
    "# Checking number of unique classes to predict in train and test set\n",
    "list_of_cat=['toxic', 'severe_toxic','obscene','threat','insult','identity_hate']\n",
    "for i in list_of_cat:\n",
    "    print('The number of unique classes in train set in '+str(i)+' class: ',train_data_full[str(i)].unique())\n",
    "    print('The number of unique classes in test set in '+str(i)+' class: ',test_data_with_labels[str(i)].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set shape:  (312735, 8)\n"
     ]
    }
   ],
   "source": [
    "full_data_set=train_data_full.append(test_data_with_labels)\n",
    "print('Full train set shape: ', full_data_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 312735 entries, 0 to 153163\n",
      "Data columns (total 8 columns):\n",
      "id               312735 non-null object\n",
      "comment_text     312735 non-null object\n",
      "toxic            312735 non-null int64\n",
      "severe_toxic     312735 non-null int64\n",
      "obscene          312735 non-null int64\n",
      "threat           312735 non-null int64\n",
      "insult           312735 non-null int64\n",
      "identity_hate    312735 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 21.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check for NA values in each columns\n",
    "full_data_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of words in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min of length of comment:  1\n",
      "Mean of length of comment:  379.7732617071962\n",
      "Median of length of comment:  193.0\n",
      "Max of length of comment:  5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xa455a63550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa455a63668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length of the word histogram in train set\n",
    "length_comments_full=full_data_set.comment_text.str.len()\n",
    "print('Min of length of comment: ',length_comments_full.min())\n",
    "print('Mean of length of comment: ',length_comments_full.mean())\n",
    "print('Median of length of comment: ',length_comments_full.median())\n",
    "print('Max of length of comment: ',length_comments_full.max())\n",
    "# Skewed to the left side- train set\n",
    "length_comments_full.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seems more than 80% of comments are of length less than 500 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to format the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def comment_format(comments, rem_stopwords=False, lemmatize=False, tokenize=False):\n",
    "# Function will return the review cleaned only with words. the numbers will be removed\n",
    "# Stopwords removal can be done using the argument rem_stopwords\n",
    "# lemmatize the words to the root word using the argument lemmatize\n",
    "\n",
    "    words_comment=re.sub(r\"[^A-Za-z]\",' ',comments)\n",
    "    \n",
    "    #Convert the text to lower case\n",
    "    words_comment_lower=words_comment.lower()\n",
    "    \n",
    "    #Split each word seperately from sentence\n",
    "    words_splitted=words_comment_lower.split()\n",
    "    meaning_full=(' '.join(words_splitted))\n",
    "    \n",
    "    # Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    if tokenize==True:\n",
    "        raw_sentences = tokenizer.tokenize(words_comment_lower.strip())\n",
    "        meaning_full=raw_sentences\n",
    "    \n",
    "    # Remove stop words\n",
    "    if rem_stopwords==True:\n",
    "        if tokenize==True:\n",
    "            stops=set(stopwords.words('english'))\n",
    "            words_rem_stops=[x for x in \" \".join(raw_sentences).split() if not x in stops]\n",
    "            meaning_full=(' '.join(words_rem_stops))\n",
    "        else:\n",
    "            stops=set(stopwords.words('english'))\n",
    "            words_rem_stops=[x for x in words_splitted if not x in stops]\n",
    "            meaning_full=(' '.join(words_rem_stops))\n",
    "        \n",
    "    #Lemmaitize each word to get meaningful words\n",
    "    if lemmatize==True:\n",
    "        if rem_stopwords==True:\n",
    "            if tokenize==True:\n",
    "                meaning_full_words_1=[lemma.lemmatize(x) for x in words_rem_stops]\n",
    "                meaning_full=(' '.join(meaning_full_words_1))\n",
    "        elif rem_stopwords==False:\n",
    "            if tokenize==True:\n",
    "                meaning_full_words_2=[lemma.lemmatize(x) for x in \" \".join(raw_sentences).split()]\n",
    "                meaning_full=(' '.join(meaning_full_words_2))\n",
    "        elif rem_stopwords==False:\n",
    "            if tokenize==False:\n",
    "                meaning_full_words_3=[lemma.lemmatize(x) for x in \" \".join(words_splitted).split()]\n",
    "                meaning_full=(' '.join(meaning_full_words_3))\n",
    "        elif rem_stopwords==True:\n",
    "            if tokenize==False:\n",
    "                meaning_full_words_4=[lemma.lemmatize(x) for x in words_rem_stops]\n",
    "                meaning_full=(' '.join(meaning_full_words_4))\n",
    "        \n",
    "    return meaning_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkin the cleaned data\n",
    "#word_test=train_data_full.comment_text[0]\n",
    "#x=comment_format(word_test, rem_stopwords=True, tokenize=False, lemmatize=True)\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in entire dataset:  312735\n",
      "____________________________________\n",
      "\n",
      " Completed Cleaning the full dataset...\n",
      "____________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the Full review data\n",
    "print('Number of reviews in entire dataset: ', full_data_set.comment_text.size)\n",
    "print('_'*36)\n",
    "\n",
    "cleaned_comment_full=[]\n",
    "for comment in full_data_set.comment_text:\n",
    "    cleaned=comment_format(comment, rem_stopwords=True) # Removing stopwords train data\n",
    "    cleaned_comment_full.append(cleaned)\n",
    "\n",
    "\n",
    "print('\\n Completed Cleaning the full dataset...')\n",
    "print('_'*36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in Test dataset:  153164\n",
      "____________________________________\n",
      "\n",
      " Completed cleaning the test dataset...\n",
      "____________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the Unknown (Test) review data\n",
    "print('Number of reviews in Test dataset: ', test_data_full.comment_text.size)\n",
    "print('_'*36)\n",
    "\n",
    "cleaned_comment_test=[]\n",
    "for comment in test_data_full.comment_text:\n",
    "    cleaned_=comment_format(comment, rem_stopwords=True) # Removing stopwords train data\n",
    "    cleaned_comment_test.append(cleaned_)\n",
    "\n",
    "\n",
    "print('\\n Completed cleaning the test dataset...')\n",
    "print('_'*36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "Shape of Train data (Count_vectorizer):  (312735, 250)\n",
      "Shape of Test data (Count_vectorizer):  (153164, 250)\n",
      "Finished....\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words - count vectorizer\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 250) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features_count_vec = vectorizer.fit_transform(cleaned_comment_full)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "#train_data_features_count_vec = train_data_features_count_vec.toarray()\n",
    "\n",
    "# Cleaning test data\n",
    "test_data_features_count_vec = vectorizer.fit_transform(cleaned_comment_test)\n",
    "#test_data_features_count_vec = test_data_features_count_vec.toarray()\n",
    "\n",
    "print('Shape of Train data (Count_vectorizer): ', train_data_features_count_vec.shape)\n",
    "print('Shape of Test data (Count_vectorizer): ', test_data_features_count_vec.shape)\n",
    "\n",
    "\n",
    "print('Finished....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Term Frequency Inverse Document Frequency (word vectorizer).....\n",
      "\n",
      "Shape of Train data (TfidF - word vectorizer):  (312735, 250)\n",
      "Shape of Test data (TfidF - word vectorizer):  (153164, 250)\n",
      "\n",
      " Finished....\n"
     ]
    }
   ],
   "source": [
    "# Creating term frequency inverse document frequency - word vectorizer\n",
    "print(\"Creating Term Frequency Inverse Document Frequency (word vectorizer).....\\n\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\\\n",
    "                                  stop_words='english', ngram_range=(1, 1), max_features=250)\n",
    "\n",
    "train_data_features_word_vec = word_vectorizer.fit_transform(cleaned_comment_full)\n",
    "#train_data_features_word_vec = train_data_features_word_vec.toarray()\n",
    "\n",
    "test_data_features_word_vec = word_vectorizer.fit_transform(cleaned_comment_test)\n",
    "#test_data_features_word_vec = test_data_features_word_vec.toarray()\n",
    "\n",
    "print('Shape of Train data (TfidF - word vectorizer): ', train_data_features_word_vec.shape)\n",
    "print('Shape of Test data (TfidF - word vectorizer): ', test_data_features_word_vec.shape)\n",
    "\n",
    "print('\\n Finished....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the vectorized features to one main feature to pass it into the model\n",
    "# Import library\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "train_features = hstack([train_data_features_count_vec, train_data_features_word_vec])\n",
    "test_features = hstack([test_data_features_count_vec, test_data_features_word_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from  sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "dv_var=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "new_df=pd.DataFrame(columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "new_df_1=pd.DataFrame(columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "nww_acc=pd.DataFrame(columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\n",
    "def get_scores(X, y, classifier, unknown_test, unknown_labels):\n",
    "    for i in dv_var:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.80)\n",
    "        model=classifier\n",
    "        class_model=model.fit(X_train, y_train[str(i)])\n",
    "        print('Training Completed for '+ str(i) + ' class')\n",
    "        y_pred=class_model.predict(X_test)\n",
    "        print('Scoring for unknown test dataset')\n",
    "        y_unknown=class_model.predict(unknown_test)\n",
    "        new_df_1[str(i)]=y_unknown\n",
    "        new_df[str(i)]=y_pred\n",
    "    print('Prediction on known data 80-20 split made')\n",
    "    accuracy_known=get_accuracy(new_df, y_test)\n",
    "    print('Prediction on unknown data')\n",
    "    accuracy_unknown=get_accuracy(new_df_1, unknown_labels)\n",
    "\n",
    "def get_accuracy(df, test_set):\n",
    "    for j in dv_var:\n",
    "        cm=confusion_matrix(test_set[str(j)], df[str(j)])\n",
    "        acc=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "        nww_acc[str(j)]=acc\n",
    "        print('Accuracy of the predictive model for '+ str(j) +': ', acc)\n",
    "    return nww_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed for toxic class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for severe_toxic class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for obscene class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for threat class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for insult class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for identity_hate class\n",
      "Scoring for unknown test dataset\n",
      "Prediction on known data 80-20 split made\n",
      "Accuracy of the predictive model for toxic:  0.6772027435368603\n",
      "Accuracy of the predictive model for severe_toxic:  0.7085072025836571\n",
      "Accuracy of the predictive model for obscene:  0.7048619438182486\n",
      "Accuracy of the predictive model for threat:  0.7114809663133324\n",
      "Accuracy of the predictive model for insult:  0.6953970614098198\n",
      "Accuracy of the predictive model for identity_hate:  0.7075319359841399\n",
      "Prediction on unknown data\n",
      "Accuracy of the predictive model for toxic:  0.41737614583061294\n",
      "Accuracy of the predictive model for severe_toxic:  0.41571779269279985\n",
      "Accuracy of the predictive model for obscene:  0.4194588806769215\n",
      "Accuracy of the predictive model for threat:  0.41654044031234494\n",
      "Accuracy of the predictive model for insult:  0.3993040139980674\n",
      "Accuracy of the predictive model for identity_hate:  0.4135436525554308\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression - Baseline\n",
    "get_scores(X=train_features, y=full_data_set, unknown_test=test_features, unknown_labels=test_data_with_labels,\\\n",
    "           classifier=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed for toxic class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for severe_toxic class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for obscene class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for threat class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for insult class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for identity_hate class\n",
      "Scoring for unknown test dataset\n",
      "Prediction on known data 80-20 split made\n",
      "Accuracy of the predictive model for toxic:  0.5077781508305754\n",
      "Accuracy of the predictive model for severe_toxic:  0.5124146641725422\n",
      "Accuracy of the predictive model for obscene:  0.5001518857818921\n",
      "Accuracy of the predictive model for threat:  0.40940412809567206\n",
      "Accuracy of the predictive model for insult:  0.48162182039106594\n",
      "Accuracy of the predictive model for identity_hate:  0.44860664780085374\n",
      "Prediction on unknown data\n",
      "Accuracy of the predictive model for toxic:  0.3561084850225902\n",
      "Accuracy of the predictive model for severe_toxic:  0.3710401922122692\n",
      "Accuracy of the predictive model for obscene:  0.3491616829019874\n",
      "Accuracy of the predictive model for threat:  0.34856101956073227\n",
      "Accuracy of the predictive model for insult:  0.34568175289232456\n",
      "Accuracy of the predictive model for identity_hate:  0.351538220469562\n"
     ]
    }
   ],
   "source": [
    "# Naive Baye's  classifier\n",
    "get_scores(X=train_features.toarray(), y=full_data_set, unknown_test=test_features.toarray(),\\\n",
    "           unknown_labels=test_data_with_labels, classifier=GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed for toxic class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for severe_toxic class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for obscene class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for threat class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for insult class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for identity_hate class\n",
      "Scoring for unknown test dataset\n",
      "Prediction on known data 80-20 split made\n",
      "Accuracy of the predictive model for toxic:  0.6371528610484916\n",
      "Accuracy of the predictive model for severe_toxic:  0.6851807440804515\n",
      "Accuracy of the predictive model for obscene:  0.6660111596079749\n",
      "Accuracy of the predictive model for threat:  0.6875629526596\n",
      "Accuracy of the predictive model for insult:  0.6651637968247878\n",
      "Accuracy of the predictive model for identity_hate:  0.6868754696468256\n",
      "Prediction on unknown data\n",
      "Accuracy of the predictive model for toxic:  0.4193674753858609\n",
      "Accuracy of the predictive model for severe_toxic:  0.45024287691624665\n",
      "Accuracy of the predictive model for obscene:  0.4375897730537202\n",
      "Accuracy of the predictive model for threat:  0.4505366796375127\n",
      "Accuracy of the predictive model for insult:  0.4405147423676582\n",
      "Accuracy of the predictive model for identity_hate:  0.4440795487190201\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "get_scores(X=train_features, y=full_data_set, unknown_test=test_features, unknown_labels=test_data_with_labels,\\\n",
    "           classifier=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed for toxic class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for severe_toxic class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for obscene class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for threat class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for insult class\n",
      "Scoring for unknown test dataset\n",
      "Training Completed for identity_hate class\n",
      "Scoring for unknown test dataset\n",
      "Prediction on known data 80-20 split made\n",
      "Accuracy of the predictive model for toxic:  0.6798567477257103\n",
      "Accuracy of the predictive model for severe_toxic:  0.7192990870865109\n",
      "Accuracy of the predictive model for obscene:  0.7042863766447631\n",
      "Accuracy of the predictive model for threat:  0.7188354357523142\n",
      "Accuracy of the predictive model for insult:  0.7035988936319888\n",
      "Accuracy of the predictive model for identity_hate:  0.7170767582777751\n",
      "Prediction on unknown data\n",
      "Accuracy of the predictive model for toxic:  0.4127340628346087\n",
      "Accuracy of the predictive model for severe_toxic:  0.4224230236870283\n",
      "Accuracy of the predictive model for obscene:  0.41488861612389333\n",
      "Accuracy of the predictive model for threat:  0.4176438327544332\n",
      "Accuracy of the predictive model for insult:  0.4221749236112925\n",
      "Accuracy of the predictive model for identity_hate:  0.4280052753910841\n"
     ]
    }
   ],
   "source": [
    "#XG-Boosting Classifier\n",
    "get_scores(X=train_features, y=full_data_set, unknown_test=test_features, unknown_labels=test_data_with_labels,\\\n",
    "           classifier=XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search for toxic class...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "param_test1 = {'C':[0.2, 0.4,0.7, 1], 'solver':['newton-cg','sag','liblinear'],'max_iter':[50, 100, 200]}\n",
    "\n",
    "log_regressor = LogisticRegression(penalty='l2', class_weight='balanced', n_jobs=4, random_state=10)\n",
    "\n",
    "dv_var=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "print(\"Grid Search for \"+dv_var[0]+\" class...\")\n",
    "gsearch1 = GridSearchCV(estimator=log_regressor, param_grid = param_test1, scoring='accuracy', n_jobs=4, iid=False, cv=5)\n",
    "gsearch1.fit(train_features, full_data_set[dv_var[0]].values.ravel())\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
